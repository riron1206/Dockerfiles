{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/jupyter_notebook/Dockerfiles/kaggle_kernel_jupyter/notebook\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/opt/conda/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pwd\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠損値（nan）の置換方法の多重代入法ためす\n",
    "- https://github.com/ghmagazine/awesomebook/blob/master/preprocess/008_number/07_d/python_awesome.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class IterativeImputer in module sklearn.impute._iterative:\n",
      "\n",
      "class IterativeImputer(sklearn.impute._base._BaseImputer)\n",
      " |  Multivariate imputer that estimates each feature from all the others.\n",
      " |  \n",
      " |  A strategy for imputing missing values by modeling each feature with\n",
      " |  missing values as a function of other features in a round-robin fashion.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <iterative_imputer>`.\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |    This estimator is still **experimental** for now: the predictions\n",
      " |    and the API might change without any deprecation cycle. To use it,\n",
      " |    you need to explicitly import ``enable_iterative_imputer``::\n",
      " |  \n",
      " |      >>> # explicitly require this experimental feature\n",
      " |      >>> from sklearn.experimental import enable_iterative_imputer  # noqa\n",
      " |      >>> # now you can import normally from sklearn.impute\n",
      " |      >>> from sklearn.impute import IterativeImputer\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object, default=BayesianRidge()\n",
      " |      The estimator to use at each step of the round-robin imputation.\n",
      " |      If ``sample_posterior`` is True, the estimator must support\n",
      " |      ``return_std`` in its ``predict`` method.\n",
      " |  \n",
      " |  missing_values : int, np.nan, default=np.nan\n",
      " |      The placeholder for the missing values. All occurrences of\n",
      " |      ``missing_values`` will be imputed.\n",
      " |  \n",
      " |  sample_posterior : boolean, default=False\n",
      " |      Whether to sample from the (Gaussian) predictive posterior of the\n",
      " |      fitted estimator for each imputation. Estimator must support\n",
      " |      ``return_std`` in its ``predict`` method if set to ``True``. Set to\n",
      " |      ``True`` if using ``IterativeImputer`` for multiple imputations.\n",
      " |  \n",
      " |  max_iter : int, default=10\n",
      " |      Maximum number of imputation rounds to perform before returning the\n",
      " |      imputations computed during the final round. A round is a single\n",
      " |      imputation of each feature with missing values. The stopping criterion\n",
      " |      is met once `abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))` < tol,\n",
      " |      where `X_t` is `X` at iteration `t. Note that early stopping is only\n",
      " |      applied if ``sample_posterior=False``.\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      Tolerance of the stopping condition.\n",
      " |  \n",
      " |  n_nearest_features : int, default=None\n",
      " |      Number of other features to use to estimate the missing values of\n",
      " |      each feature column. Nearness between features is measured using\n",
      " |      the absolute correlation coefficient between each feature pair (after\n",
      " |      initial imputation). To ensure coverage of features throughout the\n",
      " |      imputation process, the neighbor features are not necessarily nearest,\n",
      " |      but are drawn with probability proportional to correlation for each\n",
      " |      imputed target feature. Can provide significant speed-up when the\n",
      " |      number of features is huge. If ``None``, all features will be used.\n",
      " |  \n",
      " |  initial_strategy : str, default='mean'\n",
      " |      Which strategy to use to initialize the missing values. Same as the\n",
      " |      ``strategy`` parameter in :class:`sklearn.impute.SimpleImputer`\n",
      " |      Valid values: {\"mean\", \"median\", \"most_frequent\", or \"constant\"}.\n",
      " |  \n",
      " |  imputation_order : str, default='ascending'\n",
      " |      The order in which the features will be imputed. Possible values:\n",
      " |  \n",
      " |      \"ascending\"\n",
      " |          From features with fewest missing values to most.\n",
      " |      \"descending\"\n",
      " |          From features with most missing values to fewest.\n",
      " |      \"roman\"\n",
      " |          Left to right.\n",
      " |      \"arabic\"\n",
      " |          Right to left.\n",
      " |      \"random\"\n",
      " |          A random order for each round.\n",
      " |  \n",
      " |  skip_complete : boolean, default=False\n",
      " |      If ``True`` then features with missing values during ``transform``\n",
      " |      which did not have any missing values during ``fit`` will be imputed\n",
      " |      with the initial imputation method only. Set to ``True`` if you have\n",
      " |      many features with no missing values at both ``fit`` and ``transform``\n",
      " |      time to save compute.\n",
      " |  \n",
      " |  min_value : float, default=None\n",
      " |      Minimum possible imputed value. Default of ``None`` will set minimum\n",
      " |      to negative infinity.\n",
      " |  \n",
      " |  max_value : float, default=None\n",
      " |      Maximum possible imputed value. Default of ``None`` will set maximum\n",
      " |      to positive infinity.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity flag, controls the debug messages that are issued\n",
      " |      as functions are evaluated. The higher, the more verbose. Can be 0, 1,\n",
      " |      or 2.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      The seed of the pseudo random number generator to use. Randomizes\n",
      " |      selection of estimator features if n_nearest_features is not None, the\n",
      " |      ``imputation_order`` if ``random``, and the sampling from posterior if\n",
      " |      ``sample_posterior`` is True. Use an integer for determinism.\n",
      " |      See :term:`the Glossary <random_state>`.\n",
      " |  \n",
      " |  add_indicator : boolean, default=False\n",
      " |      If True, a :class:`MissingIndicator` transform will stack onto output\n",
      " |      of the imputer's transform. This allows a predictive estimator\n",
      " |      to account for missingness despite imputation. If a feature has no\n",
      " |      missing values at fit/train time, the feature won't appear on\n",
      " |      the missing indicator even if there are missing values at\n",
      " |      transform/test time.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  initial_imputer_ : object of type :class:`sklearn.impute.SimpleImputer`\n",
      " |      Imputer used to initialize the missing values.\n",
      " |  \n",
      " |  imputation_sequence_ : list of tuples\n",
      " |      Each tuple has ``(feat_idx, neighbor_feat_idx, estimator)``, where\n",
      " |      ``feat_idx`` is the current feature to be imputed,\n",
      " |      ``neighbor_feat_idx`` is the array of other features used to impute the\n",
      " |      current feature, and ``estimator`` is the trained estimator used for\n",
      " |      the imputation. Length is ``self.n_features_with_missing_ *\n",
      " |      self.n_iter_``.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iteration rounds that occurred. Will be less than\n",
      " |      ``self.max_iter`` if early stopping criterion was reached.\n",
      " |  \n",
      " |  n_features_with_missing_ : int\n",
      " |      Number of features with missing values.\n",
      " |  \n",
      " |  indicator_ : :class:`sklearn.impute.MissingIndicator`\n",
      " |      Indicator used to add binary indicators for missing values.\n",
      " |      ``None`` if add_indicator is False.\n",
      " |  \n",
      " |  random_state_ : RandomState instance\n",
      " |      RandomState instance that is generated either from a seed, the random\n",
      " |      number generator or by `np.random`.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SimpleImputer : Univariate imputation of missing values.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.experimental import enable_iterative_imputer  \n",
      " |  >>> from sklearn.impute import IterativeImputer\n",
      " |  >>> imp_mean = IterativeImputer(random_state=0)\n",
      " |  >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
      " |  IterativeImputer(random_state=0)\n",
      " |  >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
      " |  >>> imp_mean.transform(X)\n",
      " |  array([[ 6.9584...,  2.       ,  3.        ],\n",
      " |         [ 4.       ,  2.6000...,  6.        ],\n",
      " |         [10.       ,  4.9999...,  9.        ]])\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  To support imputation in inductive mode we store each feature's estimator\n",
      " |  during the ``fit`` phase, and predict without refitting (in order) during\n",
      " |  the ``transform`` phase.\n",
      " |  \n",
      " |  Features which contain all missing values at ``fit`` are discarded upon\n",
      " |  ``transform``.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \"mice:\n",
      " |      Multivariate Imputation by Chained Equations in R\". Journal of\n",
      " |      Statistical Software 45: 1-67.\n",
      " |      <https://www.jstatsoft.org/article/view/v045i03>`_\n",
      " |  \n",
      " |  .. [2] `S. F. Buck, (1960). \"A Method of Estimation of Missing Values in\n",
      " |      Multivariate Data Suitable for use with an Electronic Computer\".\n",
      " |      Journal of the Royal Statistical Society 22(2): 302-306.\n",
      " |      <https://www.jstor.org/stable/2984099>`_\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      IterativeImputer\n",
      " |      sklearn.impute._base._BaseImputer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator=None, missing_values=nan, sample_posterior=False, max_iter=10, tol=0.001, n_nearest_features=None, initial_strategy='mean', imputation_order='ascending', skip_complete=False, min_value=None, max_value=None, verbose=0, random_state=None, add_indicator=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fits the imputer on X and return self.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Input data, where \"n_samples\" is the number of samples and\n",
      " |          \"n_features\" is the number of features.\n",
      " |      \n",
      " |      y : ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fits the imputer on X and return the transformed X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Input data, where \"n_samples\" is the number of samples and\n",
      " |          \"n_features\" is the number of features.\n",
      " |      \n",
      " |      y : ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape (n_samples, n_features)\n",
      " |          The imputed input data.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Imputes all missing values in X.\n",
      " |      \n",
      " |      Note that this is stochastic, and that if random_state is not fixed,\n",
      " |      repeated calls, or permuted input, will yield different results.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input data to complete.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape (n_samples, n_features)\n",
      " |           The imputed input data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fancyimpute import IterativeImputer as MICE\n",
    "help(MICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.95847623,  2.        ,  3.        ],\n",
       "       [ 4.        ,  2.6000004 ,  6.        ],\n",
       "       [10.        ,  4.99999933,  9.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skleranに欠損値補完APIあった。\n",
    "# 以下のhelp(MICE)のまんまだと単一代入法の線形回帰で補完する？\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "imp_mean = IterativeImputer(random_state=0)\n",
    "imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
    "IterativeImputer(random_state=0)\n",
    "X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
    "imp_mean.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 nan 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 nan 0.3]\n",
      " [5.  nan 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 nan 0.1]]\n",
      "[[5.1 3.  1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 4.4 0.3]\n",
      " [5.  3.  1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 4.4 0.1]]\n"
     ]
    }
   ],
   "source": [
    "# https://www.haya-programming.com/entry/2019/10/13/025136\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 再現性確保\n",
    "np.random.seed(0)\n",
    "\n",
    "# irisでやることにする\n",
    "iris = load_iris()\n",
    "X = iris.data.copy()\n",
    "\n",
    "# 全体の約1割のデータをnanに置き換える\n",
    "mask = ~np.random.randint(10, size=X.shape).astype(np.bool)\n",
    "X[mask] = np.nan\n",
    "print(X[:10])  # 表示\n",
    "\n",
    "# モデルの定義. デフォルトだと欠損値部分は列の平均値で埋められます\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# トランスフォーム\n",
    "X_imputed = imp.fit_transform(X)\n",
    "print(X_imputed[:10])  # 表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# \n",
    "# df = sns.load_dataset('titanic')\n",
    "# imp = SimpleImputer(strategy=\"median\")\n",
    "# X_imputed = imp.fit_transform(df.values)\n",
    "# print(X_imputed[:10])  # 表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skleranに欠損値補完APIあった。\n",
    "# 単一代入法は機能しそう。多重代入法は欠損データの標準偏差必要らしいのでむずそう\n",
    "# https://www.haya-programming.com/entry/2019/11/05/225424\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=500, n_jobs=-1)\n",
    "imp = IterativeImputer(estimator=rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     type_B  type_C  type_D  type_E  fault_flg_True\n",
      "0         0       0       0       1               0\n",
      "1         0       0       1       0               0\n",
      "2         0       0       0       1               0\n",
      "3         1       0       0       0               0\n",
      "4         1       0       0       0               0\n",
      "..      ...     ...     ...     ...             ...\n",
      "995       0       1       0       0               0\n",
      "996       0       0       1       0               0\n",
      "997       1       0       0       0               0\n",
      "998       0       0       1       0               0\n",
      "999       0       1       0       0               1\n",
      "\n",
      "[1000 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([274.02738271,  40.24113136,   0.        ,   0.        ,\n",
       "         0.        ,   1.        ,   0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "production_miss_num = pd.read_csv('../input/production_missing_num.csv')\n",
    "\n",
    "# 下の行から本書スタート\n",
    "# from fancyimpute import MICE は使えない。。。\n",
    "# https://stackoverflow.com/questions/54059964/can-not-use-mice-from-fancyimputer-python\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "\n",
    "# replace関数によって、Noneをnanに変換\n",
    "production_miss_num.replace('None', np.nan, inplace=True)\n",
    "\n",
    "# mice関数を利用するためにデータ型を変換（mice関数内でモデル構築をするため）\n",
    "production_miss_num['thickness'] = production_miss_num['thickness'].astype('float64')\n",
    "production_miss_num['type'] = production_miss_num['type'].astype('category')\n",
    "production_miss_num['fault_flg'] = production_miss_num['fault_flg'].astype('category')\n",
    "\n",
    "# ダミー変数化（「第9章 カテゴリ型」で詳しく解説)\n",
    "production_dummy_flg = pd.get_dummies(production_miss_num[['type', 'fault_flg']], drop_first=True)\n",
    "print(production_dummy_flg)\n",
    "\n",
    "# mice関数にPMMを指定して、多重代入法を実施\n",
    "# n_imputationsは取得するデータセットの数\n",
    "# n_burn_inは値を取得する前に試行する回数\n",
    "# mice = MICE(n_imputations=10, n_burn_in=50, impute_type='pmm')  # エラーになる\n",
    "\n",
    "## 処理内部でTensorFlowを利用\n",
    "#production_mice = mice.multiple_imputations(\n",
    "#  # 数値の列とダミー変数を連結\n",
    "#  pd.concat([production_miss_num[['length', 'thickness']],\n",
    "#             production_dummy_flg], axis=1)\n",
    "#)\n",
    "\n",
    "production_mice = MICE().fit_transform(\n",
    "    pd.concat([production_miss_num[['length', 'thickness']], production_dummy_flg], axis=1))\n",
    "\n",
    "# 下記に補完する値が格納されている\n",
    "production_mice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
