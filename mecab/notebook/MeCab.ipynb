{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/jupyter_notebook/Dockerfiles/mecab/notebook\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/usr/local/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pwd\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 形態素解析のライブラリMeCab試す\n",
    "\n",
    "## Docker環境構築参考\n",
    "- https://kazuki-hayakawa.hatenablog.com/entry/2019/01/18/122250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       " 'HOSTNAME': 'e210238289a5',\n",
       " 'LANG': 'C.UTF-8',\n",
       " 'GPG_KEY': '<hidden>',\n",
       " 'PYTHON_VERSION': '3.6.10',\n",
       " 'PYTHON_PIP_VERSION': '20.1',\n",
       " 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/1fe530e9e3d800be94e04f6428460fc4fb94f5a9/get-pip.py',\n",
       " 'PYTHON_GET_PIP_SHA256': 'ce486cddac44e99496a702aa5c06c5028414ef48fdfd5242cd2fe559b13d4348',\n",
       " 'HOME': '/root',\n",
       " 'KERNEL_LAUNCH_TIMEOUT': '40',\n",
       " 'JPY_PARENT_PID': '1',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipから入れたnatto-pyは辞書設定なしでMeCab使えた\n",
    "\n",
    "## 形態素解析\n",
    "- 文章を名詞/動詞/副詞などの単語に辞書データを用いて分解する自然言語処理の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "メロスは激怒した。必ず、かの邪智暴虐の王を除かなければならぬと決意した。メロスには政治がわからぬ。メロスは、村の牧人である。笛を吹き、羊と遊んで暮して来た。けれども邪悪に対しては、人一倍に敏感であった。きょう未明メロスは村を出発し、野を越え山越え、十里はなれた此のシラクスの市にやって来た。メロスには父も、母も無い。女房も無い。十六の、内気な妹と二人暮しだ。この妹は、村の或る律気な一牧人を、近々、花婿として迎える事になっていた。結婚式も間近かなのである。メロスは、それゆえ、花嫁の衣裳やら祝宴の御馳走やらを買いに、はるばる市にやって来たのだ。先ず、その品々を買い集め、それから都の大路をぶらぶら歩いた。メロスには竹馬の友があった。セリヌンティウスである。今は此のシラクスの市で、石工をしている。その友を、これから訪ねてみるつもりなのだ。久しく逢わなかったのだから、訪ねて行くのが楽しみである。歩いているうちにメロスは、まちの様子を怪しく思った。ひっそりしている。もう既に日も落ちて、まちの暗いのは当りまえだが、けれども、なんだか、夜のせいばかりでは無く、市全体が、やけに寂しい。のんきなメロスも、だんだん不安になって来た。路で逢った若い衆をつかまえて、何かあったのか、二年まえに此の市に来たときは、夜でも皆が歌をうたって、まちは賑やかであった筈だが、と質問した。若い衆は、首を振って答えなかった。しばらく歩いて老爺に逢い、こんどはもっと、語勢を強くして質問した。老爺は答えなかった。メロスは両手で老爺のからだをゆすぶって質問を重ねた。老爺は、あたりをはばかる低声で、わずか答えた。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['メロス',\n",
       " '激怒',\n",
       " 'し',\n",
       " '邪智',\n",
       " '暴虐',\n",
       " '王',\n",
       " '除か',\n",
       " 'なら',\n",
       " '決意',\n",
       " 'し',\n",
       " 'メロス',\n",
       " '政治',\n",
       " 'わから',\n",
       " 'メロス',\n",
       " '村',\n",
       " '牧人',\n",
       " '笛',\n",
       " '吹き',\n",
       " '羊',\n",
       " '遊ん',\n",
       " '暮し',\n",
       " '来',\n",
       " '邪悪',\n",
       " '人一倍',\n",
       " '敏感',\n",
       " 'きょう',\n",
       " '未明',\n",
       " 'メロス',\n",
       " '村',\n",
       " '出発',\n",
       " 'し',\n",
       " '野',\n",
       " '越え',\n",
       " '山越え',\n",
       " '十',\n",
       " '里',\n",
       " 'なれ',\n",
       " '此',\n",
       " 'シラクス',\n",
       " '市',\n",
       " 'やって来',\n",
       " 'メロス',\n",
       " '父',\n",
       " '母',\n",
       " '女房',\n",
       " '十',\n",
       " '六',\n",
       " '内気',\n",
       " '妹',\n",
       " '二',\n",
       " '人',\n",
       " '暮し',\n",
       " '妹',\n",
       " '村',\n",
       " '律',\n",
       " '気',\n",
       " '一',\n",
       " '牧人',\n",
       " '花婿',\n",
       " '迎える',\n",
       " '事',\n",
       " 'なっ',\n",
       " 'い',\n",
       " '結婚式',\n",
       " '間近',\n",
       " 'の',\n",
       " 'メロス',\n",
       " 'それ',\n",
       " 'ゆえ',\n",
       " '花嫁',\n",
       " '衣裳',\n",
       " '祝宴',\n",
       " '御馳走',\n",
       " '買い',\n",
       " '市',\n",
       " 'やって来',\n",
       " 'の',\n",
       " '品々',\n",
       " '買い',\n",
       " '集め',\n",
       " '都',\n",
       " '大路',\n",
       " '歩い',\n",
       " 'メロス',\n",
       " '竹馬',\n",
       " '友',\n",
       " 'あっ',\n",
       " 'セリヌンティウス',\n",
       " '今',\n",
       " '此',\n",
       " 'シラクス',\n",
       " '市',\n",
       " '石工',\n",
       " 'し',\n",
       " 'いる',\n",
       " '友',\n",
       " '訪ね',\n",
       " 'みる',\n",
       " 'つもり',\n",
       " 'の',\n",
       " '逢わ',\n",
       " 'の',\n",
       " '訪ね',\n",
       " '行く',\n",
       " 'の',\n",
       " '楽しみ',\n",
       " '歩い',\n",
       " 'いる',\n",
       " 'うち',\n",
       " 'メロス',\n",
       " 'まち',\n",
       " '様子',\n",
       " '思っ',\n",
       " 'し',\n",
       " 'いる',\n",
       " '日',\n",
       " '落ち',\n",
       " 'まち',\n",
       " 'の',\n",
       " '当り',\n",
       " 'まえ',\n",
       " '夜',\n",
       " 'せい',\n",
       " '市',\n",
       " '全体',\n",
       " 'のんき',\n",
       " 'メロス',\n",
       " '不安',\n",
       " 'なっ',\n",
       " '来',\n",
       " '路',\n",
       " '逢っ',\n",
       " '若い衆',\n",
       " 'つかまえ',\n",
       " '何',\n",
       " 'あっ',\n",
       " 'の',\n",
       " '二',\n",
       " '年',\n",
       " 'まえ',\n",
       " '此',\n",
       " '市',\n",
       " '来',\n",
       " 'とき',\n",
       " '夜',\n",
       " '皆',\n",
       " '歌',\n",
       " 'うたっ',\n",
       " 'まち',\n",
       " '賑やか',\n",
       " '筈',\n",
       " '質問',\n",
       " 'し',\n",
       " '若い衆',\n",
       " '首',\n",
       " '振っ',\n",
       " '答え',\n",
       " '歩い',\n",
       " '老爺',\n",
       " '逢い',\n",
       " 'こんど',\n",
       " '語勢',\n",
       " 'し',\n",
       " '質問',\n",
       " 'し',\n",
       " '老爺',\n",
       " '答え',\n",
       " 'メロス',\n",
       " '両手',\n",
       " '老爺',\n",
       " 'からだ',\n",
       " 'ゆすぶっ',\n",
       " '質問',\n",
       " '重ね',\n",
       " '老爺',\n",
       " 'あたり',\n",
       " 'はばかる',\n",
       " '声',\n",
       " '答え']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/ghmagazine/awesomebook/blob/master/preprocess/011_character/01/python_awesome.py\n",
    "\n",
    "import os\n",
    "from natto import MeCab\n",
    "\n",
    "# merosには、メロスの文章データが格納\n",
    "# MeCabを実行するオブジェクトを生成\n",
    "mc = MeCab()\n",
    "\n",
    "# 下記のコードはテキスト時は、下記のようにする\n",
    "with open('../input/txt/meros.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "print(txt)\n",
    "    \n",
    "word_list = []\n",
    "# MeCabを用いて、形態素解析を実行\n",
    "for part_and_word in mc.parse(txt, as_nodes=True):\n",
    "    # 形態素解析結果のpart_and_wordが開始/終了オブジェクトでないことを判定\n",
    "    if not (part_and_word.is_bos() or part_and_word.is_eos()):\n",
    "\n",
    "        # 形態素解析結果から品詞と単語を取得\n",
    "        part, word = part_and_word.feature.split(',', 1)\n",
    "\n",
    "        # 名詞と動詞の単語を抽出\n",
    "        if part == '名詞' or part == '動詞':\n",
    "            word_list.append(part_and_word.surface)\n",
    "            \n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### pipから入れたmecab-python3については別途辞書が必要\n",
    "```\n",
    "$ cd ../input\n",
    "$ git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "さらに $ ./bin/install-mecab-ipadic-neologd -n -y でインストール必要。README.ja.md より2GB以上データとられるのでやめといた\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPYING  ChangeLog  README.ja.md  README.md  bin  diff\tlibexec  misc  seed\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/mecab-ipadic-neologd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MeCab\n",
    "# \n",
    "# wakati = MeCab.Tagger('-Owakati')    #分かち書き\n",
    "# neo_wakati = MeCab.Tagger('-Owakati -d ../input/mecab-ipadic-neologd') #追加辞書を適用\n",
    "# word = input(\"分かち書き：\")\n",
    "# \n",
    "# wakati = wakati.parse(word).strip()\n",
    "# neo_wakati = neo_wakati.parse(word).strip()\n",
    "# \n",
    "# print('通常辞書：' + wakati)\n",
    "# print('追加辞書：' + neo_wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of wordsの作成\n",
    "- 文章に含まれる単語の種類ごとの指標を数値化する（単語の出現回数の表を作るような）前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<232x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 253 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/ghmagazine/awesomebook/blob/master/preprocess/011_character/02/python_awesome.py\n",
    "\n",
    "import os\n",
    "from natto import MeCab\n",
    "# bug of wordsを作成するためのライブラリ読み込み\n",
    "from gensim import corpora, matutils\n",
    "\n",
    "mc = MeCab()\n",
    "txt_word_list = []\n",
    "\n",
    "# テキストファイルを格納しているフォルダを読み込み\n",
    "files = os.listdir('../input/txt')\n",
    "\n",
    "# フォルダ配下のテキストファイルを1つずつ読み込み\n",
    "for file in files:\n",
    "    # テキストファイルから名詞と動詞の単語を取り出したリスト作成（Q11-1の処理と同じ）\n",
    "    with open('../input/txt/'+file, 'r') as f:\n",
    "        txt = f.read()\n",
    "        word_list = []\n",
    "    for n in mc.parse(txt, as_nodes=True):\n",
    "        if not (n.is_bos() or n.is_eos()):\n",
    "            part, word = n.feature.split(',', 1)\n",
    "        if part == \"名詞\" or part == \"動詞\":\n",
    "            word_list.append(n.surface)\n",
    "\n",
    "    # テキストファイルごとの単語リストを追加\n",
    "    txt_word_list.append(word_list)\n",
    "\n",
    "# bug of wordsを作成するため全種類の単語を把握し、単語IDを付与した辞書を作成\n",
    "corpus_dic = corpora.Dictionary(txt_word_list)\n",
    "\n",
    "# 各文章の単語リストをコーパス（辞書の単語IDと単語の出現回数）リストに変換\n",
    "corpus_list = [corpus_dic.doc2bow(word_in_text) for word_in_text in txt_word_list]\n",
    "\n",
    "# コーパスリストをスパースマトリックス（csc型）に変換\n",
    "word_matrix = matutils.corpus2csc(corpus_list)\n",
    "\n",
    "word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFによる単語の重要度調整\n",
    "- 単語の重要度（=ある単語が文章の特徴づけにどの程度影響力があるのかを示した値）を表す指標を計算\n",
    "- Trem Frequency(文章中の単語の出現割合=[対象の単語の数] / [文章の単語の総数]) - Inverse Document Frequency(文章内の単語の出現割合によるスコア=log([全文章数] / [対象の単語が出現している文章数] + 1))\n",
    "- TF * IDFの値が単語の重要度。重要な単語ほど値が大きくなる\n",
    "- 文章によって長さが違うため、TF-IDFは正規化が必要。 →文章ごとに、TF-IDFの2乗の合計を1にそろえるL2ノルムという手法がよく使われる。（ここでいう文章の単位は小説や論文など。例えば「走れメロス」1冊が1文書と考える）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<232x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 238 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/ghmagazine/awesomebook/blob/master/preprocess/011_character/03/python_awesome.py\n",
    "\n",
    "from natto import MeCab\n",
    "import os\n",
    "from gensim import corpora\n",
    "\n",
    "mc = MeCab()\n",
    "\n",
    "txt_list = []\n",
    "files = os.listdir('../input/txt')\n",
    "for file in files:\n",
    "    with open('../input/txt/'+file, 'r') as f:\n",
    "        txt = f.read()\n",
    "        word_list = []\n",
    "        for n in mc.parse(txt, as_nodes=True):\n",
    "            if not (n.is_bos() or n.is_eos()):\n",
    "                part, word = n.feature.split(',', 1)\n",
    "                if part == \"名詞\" or part == \"動詞\":\n",
    "                    word_list.append(n.surface)\n",
    "        txt_list.append(word_list)\n",
    "\n",
    "dictionary = corpora.Dictionary(txt_list)\n",
    "corpus_list = [dictionary.doc2bow(txt) for txt in txt_list]\n",
    "\n",
    "# 下の行から本書スタート\n",
    "from gensim import matutils, models\n",
    "\n",
    "# corpus_listを準備するコードは省略\n",
    "\n",
    "# TF-IDFのモデルを生成\n",
    "tfidf_model = models.TfidfModel(corpus_list, normalize=True)\n",
    "\n",
    "# corpusにTF-IDFを適用\n",
    "corpus_list_tfidf = tfidf_model[corpus_list]\n",
    "word_matrix = matutils.corpus2csc(corpus_list_tfidf)\n",
    "\n",
    "word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
